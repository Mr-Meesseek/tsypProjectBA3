{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687f893",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497965aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\" : \"system\", \"content\": \"You will get a message from a user, you will take that message and deduct the general topic, limit the response to a maximum of five words\"},\n",
    "             {\"role\" : \"user\", \"content\": \"Today I feel like asking questions about Recurrent neural networks and their effect within the text analysis subject\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876abd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=\"llama3:8b\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d3285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ba268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Analysis RNN Effect\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewing = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "You are Mr. Ollama, an interviewer. \n",
    "You will ask one multiple-choice question (QCM) with a single correct answer.\n",
    "The topic is: {topic}.\n",
    "\n",
    "Your entire output must be **valid JSON**, with **no text or formatting before or after**.\n",
    "Use this exact structure:\n",
    "\n",
    "{{\n",
    "  \"question\": \"Your question here, including options A), B), C), D).\",\n",
    "  \"answer\": \"A\"  // The correct option letter, always as a string\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- The JSON must be strictly valid according to RFC 8259.\n",
    "- Always enclose strings in double quotes.\n",
    "- Never include code fences, markdown, explanations, or comments.\n",
    "- Only output one JSON object.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QuestionsAskedByOllama = ollama.chat(messages=interviewing, model=\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"Which effect is often observed when using Recurrent Neural Networks (RNNs) for text analysis?\",\n",
      "  \"answer\": \"A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(QuestionsAskedByOllama[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c768a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='{\\n  \"question\": \"Which effect is often observed when using Recurrent Neural Networks (RNNs) for text analysis?\",\\n  \"answer\": \"A\"\\n}' thinking=None images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "print(QuestionsAskedByOllama[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"Which effect is often observed when using Recurrent Neural Networks (RNNs) for text analysis?\",\n",
      "  \"answer\": \"A\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(QuestionsAskedByOllama[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = json.loads(QuestionsAskedByOllama[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451aca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewingAnswer = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are Mr. Ollama, an interviewer continuing a multiple-choice test (QCM).\n",
    "\n",
    "You will receive two inputs:\n",
    "1. A JSON object containing the original question and the correct answer.\n",
    "2. A user's sentence or response (which may include or imply a choice like \"I think it's A\", \"The answer is Overfitting\", etc.).\n",
    "\n",
    "Your job is to:\n",
    "- Identify which choice (A, B, C, or D) the user selected or implied.\n",
    "- Compare it with the correct answer.\n",
    "- Respond as the interviewer, saying whether the user is right or wrong, and briefly explain the correct answer.\n",
    "\n",
    "Your output must be **valid JSON only**, with the following exact structure:\n",
    "\n",
    "{\n",
    "  \"detected_answer\": \"A\",  // The choice letter you think the user meant\n",
    "  \"correct_answer\": \"A\",   // The correct choice from the question\n",
    "  \"is_correct\": true,      // true or false\n",
    "  \"interviewer_response\": \"Correct! Overfitting is indeed a common challenge in text analysis neural networks.\"\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Always use double quotes for strings.\n",
    "- Never output anything outside the JSON.\n",
    "- If you cannot determine the userâ€™s answer, set \"detected_answer\" to \"unknown\" and \"is_correct\" to false.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ecba5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"detected_answer\": \"B\",\n",
      "  \"correct_answer\": \"A\",\n",
      "  \"is_correct\": false,\n",
      "  \"interviewer_response\": \"That's not correct. Overfitting is a common challenge when training a text analysis neural network, as the model may learn to fit the noise in the training data rather than the underlying patterns.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question_data = {\n",
    "    \"question\": \"What is a common challenge when training a text analysis neural network? A) Overfitting B) Underfitting C) Class imbalance D) Data scarcity.\",\n",
    "    \"answer\": \"A\"\n",
    "}\n",
    "\n",
    "user_sentence = \"I think it's underfitting.\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3:8b\",\n",
    "    messages=[\n",
    "        *interviewingAnswer,\n",
    "        {\"role\": \"user\", \"content\": json.dumps({\n",
    "            \"question_data\": question_data,\n",
    "            \"user_response\": user_sentence\n",
    "        })}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import re\n",
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "import pygame\n",
    "import time\n",
    "import threading\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "\n",
    "pygame.mixer.init()\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Convert text to speech and play it - non-blocking version\"\"\"\n",
    "    def _tts_thread(text):\n",
    "        try:\n",
    "            mp3_fp = BytesIO()\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            tts.write_to_fp(mp3_fp)\n",
    "            mp3_fp.seek(0)\n",
    "            \n",
    "            pygame.mixer.music.load(mp3_fp, 'mp3')\n",
    "            pygame.mixer.music.play()\n",
    "            \n",
    "            while pygame.mixer.music.get_busy():\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Audio error: {e}\")\n",
    "    \n",
    "    threading.Thread(target=_tts_thread, args=(text,), daemon=True).start()\n",
    "\n",
    "def generate_questions_optimized(topic, num_questions=5):\n",
    "    \"\"\"Generate questions with optimized prompt and settings\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Generate exactly {num_questions} multiple-choice questions about {topic}.\n",
    "Return ONLY valid JSON in this exact format:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"Clear question here?\",\n",
    "    \"options\": {{\n",
    "      \"A\": \"Option A\",\n",
    "      \"B\": \"Option B\", \n",
    "      \"C\": \"Option C\",\n",
    "      \"D\": \"Option D\"\n",
    "    }},\n",
    "    \"answer\": \"A\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Requirements:\n",
    "- Only JSON output, no other text\n",
    "- {num_questions} questions total\n",
    "- Each question must be concise (max 15 words)\n",
    "- Options should be short (max 8 words each)\n",
    "- Ensure answers are factually correct\n",
    "- Cover different aspects of {topic}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",  \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": 0.3,  \n",
    "                \"num_predict\": 512,  \n",
    "            }\n",
    "        )\n",
    "        \n",
    "        content = response[\"message\"][\"content\"].strip()\n",
    "        content = re.sub(r'```json\\s*|\\s*```', '', content)\n",
    "        \n",
    "        questions = json.loads(content)\n",
    "        \n",
    "        for q in questions:\n",
    "            if not all(k in q for k in ['question', 'options', 'answer']):\n",
    "                raise ValueError(\"Invalid question structure\")\n",
    "                \n",
    "        return questions[:num_questions]  \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Question generation failed: {e}\")\n",
    "        return generate_fallback_questions(topic, num_questions)\n",
    "\n",
    "def generate_fallback_questions(topic, num_questions):\n",
    "    \"\"\"Fallback questions if generation fails\"\"\"\n",
    "    print(\"ğŸ”„ Using fallback questions...\")\n",
    "    return [\n",
    "        {\n",
    "            \"question\": f\"What is the main concept of {topic}?\",\n",
    "            \"options\": {\n",
    "                \"A\": \"Fundamental principle\",\n",
    "                \"B\": \"Advanced technique\", \n",
    "                \"C\": \"Basic terminology\",\n",
    "                \"D\": \"Historical context\"\n",
    "            },\n",
    "            \"answer\": \"A\"\n",
    "        }\n",
    "    ] * min(num_questions, 3)\n",
    "\n",
    "def evaluate_answer_optimized(question_data, user_answer):\n",
    "    \"\"\"Fast answer evaluation without LLM call\"\"\"\n",
    "    \n",
    "    correct_answer = question_data.get('answer', '').upper()\n",
    "    user_answer = user_answer.upper()\n",
    "    \n",
    "    is_correct = user_answer == correct_answer\n",
    "    \n",
    "    if is_correct:\n",
    "        feedback = \"Correct! Well done.\"\n",
    "    else:\n",
    "        correct_option = question_data['options'].get(correct_answer, 'the correct option')\n",
    "        feedback = f\"Incorrect. The right answer was {correct_answer}: {correct_option}\"\n",
    "    \n",
    "    return {\n",
    "        \"detected_answer\": user_answer,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"interviewer_response\": feedback\n",
    "    }\n",
    "\n",
    "def preload_next_audio(question_data, question_num):\n",
    "    \"\"\"Preload audio for next question while current one is being answered\"\"\"\n",
    "    question_text = question_data.get('question', '')\n",
    "    options = question_data.get('options', {})\n",
    "    \n",
    "    audio_text = f\"Question {question_num}. {question_text}. Options: \"\n",
    "    \n",
    "    for key, value in options.items():\n",
    "        short_option = value[:50]  \n",
    "        audio_text += f\"{key}. {short_option}. \"\n",
    "    \n",
    "\n",
    "    text_to_speech(audio_text)\n",
    "\n",
    "def run_optimized_interview():\n",
    "    user_message = input(\"ğŸ‘‹ What topic would you like to be interviewed on? \")\n",
    "    \n",
    "\n",
    "    topic = user_message.split()[0] if user_message else \"general knowledge\"\n",
    "    print(f\"ğŸ¯ Topic: {topic}\")\n",
    "    \n",
    "    text_to_speech(f\"Starting interview on {topic}\")\n",
    "    \n",
    "   \n",
    "    print(\"ğŸ”„ Generating questions (optimized)...\")\n",
    "    all_questions = generate_questions_optimized(topic, num_questions=3)  \n",
    "    \n",
    "    if not all_questions:\n",
    "        print(\"âŒ Failed to generate questions. Please try again.\")\n",
    "        return\n",
    "    \n",
    "    score = 0\n",
    "    total_questions = len(all_questions)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Starting interview with {total_questions} questions...\")\n",
    "    \n",
    "    if total_questions > 0:\n",
    "        preload_next_audio(all_questions[0], 1)\n",
    "    \n",
    "    for i, question in enumerate(all_questions, start=1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"ğŸ§  Question {i}/{total_questions}:\")\n",
    "        print(f\"Q: {question['question']}\")\n",
    "        \n",
    "        for key, value in question['options'].items():\n",
    "            print(f\"   {key}) {value}\")\n",
    "        \n",
    "        user_answer = input(\"\\nğŸ‘‰ Your answer (A/B/C/D): \").strip().upper()\n",
    "        \n",
    "        result = evaluate_answer_optimized(question, user_answer)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Result:\")\n",
    "        print(f\"   Your answer: {result['detected_answer']}\")\n",
    "        print(f\"   Correct answer: {result['correct_answer']}\")\n",
    "        print(f\"   ğŸ—£ï¸ {result['interviewer_response']}\")\n",
    "        \n",
    "        feedback_text = \"Correct! \" if result['is_correct'] else f\"Incorrect. Correct was {result['correct_answer']}.\"\n",
    "        text_to_speech(feedback_text)\n",
    "        \n",
    "        if result['is_correct']:\n",
    "            score += 1\n",
    "            print(\"   âœ… Correct!\")\n",
    "        else:\n",
    "            print(\"   âŒ Incorrect\")\n",
    "        \n",
    "        if i < total_questions:\n",
    "            preload_next_audio(all_questions[i], i + 1)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"ğŸ Interview Finished!\")\n",
    "    print(f\"ğŸ“Š Final Score: {score}/{total_questions}\")\n",
    "    \n",
    "    percentage = (score / total_questions) * 100\n",
    "    final_text = f\"You scored {score} out of {total_questions}.\"\n",
    "    \n",
    "    if percentage >= 80:\n",
    "        final_text += \" Excellent work!\"\n",
    "    elif percentage >= 60:\n",
    "        final_text += \" Good job!\"\n",
    "    else:\n",
    "        final_text += \" Keep practicing!\"\n",
    "    \n",
    "    text_to_speech(final_text)\n",
    "    print(f\"ğŸ¯ {final_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ce21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Topic: Convolutional\n",
      "ğŸ”„ Generating questions (optimized)...\n",
      "\n",
      "ğŸ“ Starting interview with 3 questions...\n",
      "\n",
      "========================================\n",
      "ğŸ§  Question 1/3:\n",
      "Q: What is the primary goal of a convolutional layer?\n",
      "   A) To perform element-wise multiplication\n",
      "   B) To extract features from input data\n",
      "   C) To apply pooling to reduce spatial dimensions\n",
      "   D) To apply normalization to input values\n",
      "âš ï¸ Audio error: Failed to connect. Probable cause: Unknown\n",
      "\n",
      "ğŸ“Š Result:\n",
      "   Your answer: B\n",
      "   Correct answer: B\n",
      "   ğŸ—£ï¸ Correct! Well done.\n",
      "   âœ… Correct!\n",
      "\n",
      "========================================\n",
      "ğŸ§  Question 2/3:\n",
      "Q: What is the main advantage of using convolutional neural networks (CNNs) over traditional neural networks?\n",
      "   A) Faster training times\n",
      "   B) Better performance on image classification tasks\n",
      "   C) Increased number of hidden layers\n",
      "   D) Improved handling of sequential data\n",
      "\n",
      "ğŸ“Š Result:\n",
      "   Your answer: C\n",
      "   Correct answer: B\n",
      "   ğŸ—£ï¸ Incorrect. The right answer was B: Better performance on image classification tasks\n",
      "   âŒ Incorrect\n",
      "\n",
      "========================================\n",
      "ğŸ§  Question 3/3:\n",
      "Q: What is the purpose of the pooling layer in a convolutional neural network?\n",
      "   A) To increase the spatial dimensions of the input data\n",
      "   B) To reduce the spatial dimensions and downsample the feature maps\n",
      "   C) To apply non-linear transformations to the input data\n",
      "   D) To normalize the output values\n",
      "\n",
      "ğŸ“Š Result:\n",
      "   Your answer: D\n",
      "   Correct answer: B\n",
      "   ğŸ—£ï¸ Incorrect. The right answer was B: To reduce the spatial dimensions and downsample the feature maps\n",
      "   âŒ Incorrect\n",
      "\n",
      "========================================\n",
      "ğŸ Interview Finished!\n",
      "ğŸ“Š Final Score: 1/3\n",
      "ğŸ¯ You scored 1 out of 3. Keep practicing!\n"
     ]
    }
   ],
   "source": [
    "run_optimized_interview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516978b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c1a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
